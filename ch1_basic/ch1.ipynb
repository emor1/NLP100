{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第1章 [準備運動](https://nlp100.github.io/ja/ch01.html)\n",
    "## 00 文字列の逆順\n",
    "文字列”stressed”の文字を逆に（末尾から先頭に向かって）並べた文字列を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desserts\n"
     ]
    }
   ],
   "source": [
    "word = \"stressed\"\n",
    "print(word[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. 「パタトクカシーー」\n",
    "「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "タクシー\n"
     ]
    }
   ],
   "source": [
    "word = \"パタトクカシーー\"\n",
    "new_word = \"\"\n",
    "for char in range(len(word)):\n",
    "    if(char%2 != 0):\n",
    "        new_word += word[char]\n",
    "print(new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "タクシー\n"
     ]
    }
   ],
   "source": [
    "# スライスを使うともっと奇麗にかける\n",
    "word = \"パタトクカシーー\"\n",
    "print(word[1::2])\n",
    "\n",
    "#でも実行時間は、僕が考えた方が少し早いみたい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. 「パトカー」＋「タクシー」＝「パタトクカシーー」\n",
    "「パトカー」＋「タクシー」の文字を先頭から交互に連結して文字列「パタトクカシーー」を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パタトクカシーー\n"
     ]
    }
   ],
   "source": [
    "word_1 = \"パトカー\"\n",
    "word_2 = \"タクシー\"\n",
    "\n",
    "word = \"\"\n",
    "for i in range(len(word_1)):\n",
    "    word += (word_1[i] + word_2[i])\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. 円周率\n",
    "“Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.”という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7]\n"
     ]
    }
   ],
   "source": [
    "text = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics\"\n",
    "\n",
    "new_text = text.replace('.', '').replace(',', '')\n",
    "\n",
    "list_word = []\n",
    "word = \"\"\n",
    "\n",
    "\n",
    "for char in new_text:    \n",
    "    if char == \" \":\n",
    "        list_word.append(len(word))\n",
    "        word = \"\"\n",
    "    else:\n",
    "        word += char\n",
    "\n",
    "print(list_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "# 参考回答\n",
    "raw_text = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics\"\n",
    "\n",
    "# replace('置換前', '置換後')\n",
    "text = raw_text.replace('.', ' ').replace(',', ' ')\n",
    "# split(): 指定なしで反核で切る\n",
    "ans = [len(w) for w in text.split()]\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. 元素記号 \n",
    "“Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.”という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭の2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 1, 'He': 2, 'Li': 3, 'Be': 4, 'B': 5, 'C': 6, 'N': 7, 'O': 8, 'F': 9, 'Ne': 10, 'Na': 11, 'Mi': 12, 'Al': 13, 'Si': 14, 'P': 15, 'S': 16, 'Cl': 17, 'Ar': 18, 'K': 19, 'Ca': 20}\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can\"\n",
    "word_list = [w for w in text.replace('.', ' ').replace(',', ' ').split()]\n",
    "arry = [1, 5, 6, 7, 8, 9, 15, 16, 19]\n",
    "\n",
    "word =[]\n",
    "index = [i for i in range(1, len(word_list)+1)]\n",
    "\n",
    "for atom in range(len(word_list)):\n",
    "    if atom+1 in arry:\n",
    "        word.append(word_list[atom][0])\n",
    "    else:\n",
    "        word.append(word_list[atom][:2])\n",
    "\n",
    "# zip関数で、要素をまとめてる\n",
    "print(dict(zip(word,index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 1, 'He': 2, 'Li': 3, 'Be': 4, 'B': 5, 'C': 6, 'N': 7, 'O': 8, 'F': 9, 'Ne': 10, 'Na': 11, 'Mi': 12, 'Al': 13, 'Si': 14, 'P': 15, 'S': 16, 'Cl': 17, 'Ar': 18, 'K': 19, 'Ca': 20}\n"
     ]
    }
   ],
   "source": [
    "# 参考回答\n",
    "def extract_chars(i, word):\n",
    "    if i in [1, 5, 6, 7, 8, 9, 15, 16, 19]:\n",
    "        return (word[0], i)\n",
    "    else:\n",
    "        return (word[:2], i)\n",
    "\n",
    "# 文を単語に分割\n",
    "raw_text = 'Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.'\n",
    "text = raw_text.replace('.', '').replace(',', '')\n",
    "# 文字を取り出す\n",
    "    # enumerateを使って、インデックス情報を取得、第二引数で０以外から開始する\n",
    "ans = [extract_chars(i, w) for i, w in enumerate(text.split(), 1)]\n",
    "# dictを用いて辞書型のデータに変換\n",
    "print(dict(ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. n-gram\n",
    "与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，”I am an NLPer”という文から単語bi-gram，文字bi-gramを得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I a', ' am', 'am ', 'm a', ' an', 'an ', 'n N', ' NL', 'NLP', 'LPe', 'Per']\n",
      "[['I', 'am'], ['am', 'an'], ['an', 'NLPer']]\n"
     ]
    }
   ],
   "source": [
    "text = \"I am an NLPer\"\n",
    "\n",
    "def n_gram1(text, type, n):\n",
    "    if type==\"w\":\n",
    "        text=text.split()\n",
    "    text = [text[idx:idx+n] for idx in range(len(text)-n+1)]\n",
    "    return text\n",
    "\n",
    "print(n_gram1(text, \"c\",3))\n",
    "print(n_gram1(text, \"w\",2))\n",
    "\n",
    "# よく分からないのでまた後でやる\n",
    "# これが参考になりそう：https://qiita.com/kazmaw/items/4df328cba6429ec210fb\n",
    "\n",
    "# \"\"\"\n",
    "# N-gramとは\n",
    "#     任意の文字列や文書を連続したnこの文字で分割する方法\n",
    "#     n=1: uni-gram, n=2: bi-gram, n=3: tri-gram\n",
    "#     ex. I an an NLPer\n",
    "#     uni-gram:==> 'I', ' ', 'a', 'm', ' ', 'a', 'n', ' ', 'N', 'L', 'P', 'e', 'r'\n",
    "#     bi-gram: ==> \n",
    "#     単語N-gramであれば、単語単位で区切る\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'a', 'm', ' ', 'a', 'n', ' ', 'N', 'L', 'P', 'e', 'r']\n",
      "[['I'], ['am'], ['an'], ['NLPer']]\n",
      "['I ', ' a', 'am', 'm ', ' a', 'an', 'n ', ' N', 'NL', 'LP', 'Pe', 'er']\n",
      "[['I', 'am'], ['am', 'an'], ['an', 'NLPer']]\n",
      "['I a', ' am', 'am ', 'm a', ' an', 'an ', 'n N', ' NL', 'NLP', 'LPe', 'Per']\n",
      "[['I', 'am', 'an'], ['am', 'an', 'NLPer']]\n"
     ]
    }
   ],
   "source": [
    "def n_gram(target, n):\n",
    "    #基準を1文字ずつずらしながらn文字抜き出している\n",
    "    return [target[idx:idx + n] for idx in range(len(target) - n + 1)]\n",
    "\n",
    "\n",
    "text = 'I am an NLPer'\n",
    "for i in range(1, 4):\n",
    "    print(n_gram(text, i))\n",
    "    print(n_gram(text.split(' '), i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. 集合\n",
    "“paraparaparadise”と”paragraph”に含まれる文字bi-gramの集合を，それぞれ, XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，’se’というbi-gramがXおよびYに含まれるかどうかを調べよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "和集合： {'is', 'ar', 'pa', 'di', 'ph', 'ad', 'ap', 'gr', 'se', 'ra', 'ag'}\n",
      "積集合： {'ap', 'ar', 'ra', 'pa'}\n",
      "差集合 {'se', 'di', 'is', 'ad'}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def n_gram1(text, type, n):\n",
    "    if type==\"w\":\n",
    "        text=text.split()\n",
    "    text = [text[idx:idx+n] for idx in range(len(text)-n+1)]\n",
    "    return text\n",
    "\n",
    "text1 = \"paraparaparadise\"\n",
    "text2 = \"paragraph\"\n",
    "\n",
    "x = n_gram1(text1, \"c\", 2)\n",
    "y = n_gram1(text2, \"c\", 2)\n",
    "\n",
    "wa_shugo = set(x)|set(y)        # 和集合\n",
    "seki_shugo = set(x)&set(y)      # 積集合\n",
    "sa_shugo = set(x)-set(y)        # 差集合\n",
    "\n",
    "print(\"和集合：\",wa_shugo)\n",
    "print(\"積集合：\",seki_shugo)\n",
    "print(\"差集合\", sa_shugo)\n",
    "\n",
    "print(\"se\" in wa_shugo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. テンプレートによる文生成\n",
    "引数x, y, zを受け取り「x時のyはz」という文字列を返す関数を実装せよ．さらに，x=12, y=”気温”, z=22.4として，実行結果を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12の気温は22.4\n"
     ]
    }
   ],
   "source": [
    "def sentence_maker(x, y, z):\n",
    "    return f\"{x}の{y}は{z}\"\n",
    "\n",
    "print(sentence_maker(12, \"気温\", 22.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08. 暗号文\n",
    "与えられた文字列の各文字を，以下の仕様で変換する関数cipherを実装せよ．\n",
    "\n",
    "英小文字ならば(219 - 文字コード)の文字に置換\n",
    "その他の文字はそのまま出力\n",
    "この関数を用い，英語のメッセージを暗号化・復号化せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あzistzrliおありtはお\n"
     ]
    }
   ],
   "source": [
    "def cipher(text):\n",
    "    return ''.join([chr(219-ord(w)) if 122>=ord(w)>=97 else w for w in text])\n",
    "\n",
    "print(cipher(\"あarhgaiorおありgはお\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09. Typoglycemia\n",
    "スペースで区切られた単語列に対して，各単語の先頭と末尾の文字は残し，それ以外の文字の順序をランダムに並び替えるプログラムを作成せよ．ただし，長さが４以下の単語は並び替えないこととする．適当な英語の文（例えば”I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .”）を与え，その実行結果を確認せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b8b7b4f0ecb4fe488c708522ab2b3d7dd4520241d9f917dd3cf1964317173b9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('.nlp100': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
