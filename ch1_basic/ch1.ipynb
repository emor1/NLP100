{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第1章 [準備運動](https://nlp100.github.io/ja/ch01.html)\n",
    "## 00 文字列の逆順\n",
    "文字列”stressed”の文字を逆に（末尾から先頭に向かって）並べた文字列を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desserts\n"
     ]
    }
   ],
   "source": [
    "word = \"stressed\"\n",
    "print(word[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. 「パタトクカシーー」\n",
    "「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "タクシー\n"
     ]
    }
   ],
   "source": [
    "word = \"パタトクカシーー\"\n",
    "new_word = \"\"\n",
    "for char in range(len(word)):\n",
    "    if(char%2 != 0):\n",
    "        new_word += word[char]\n",
    "print(new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "タクシー\n"
     ]
    }
   ],
   "source": [
    "# スライスを使うともっと奇麗にかける\n",
    "word = \"パタトクカシーー\"\n",
    "print(word[1::2])\n",
    "\n",
    "#でも実行時間は、僕が考えた方が少し早いみたい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. 「パトカー」＋「タクシー」＝「パタトクカシーー」\n",
    "「パトカー」＋「タクシー」の文字を先頭から交互に連結して文字列「パタトクカシーー」を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パタトクカシーー\n"
     ]
    }
   ],
   "source": [
    "word_1 = \"パトカー\"\n",
    "word_2 = \"タクシー\"\n",
    "\n",
    "word = \"\"\n",
    "for i in range(len(word_1)):\n",
    "    word += (word_1[i] + word_2[i])\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. 円周率\n",
    "“Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.”という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7]\n"
     ]
    }
   ],
   "source": [
    "text = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics\"\n",
    "\n",
    "new_text = text.replace('.', '').replace(',', '')\n",
    "\n",
    "list_word = []\n",
    "word = \"\"\n",
    "\n",
    "\n",
    "for char in new_text:    \n",
    "    if char == \" \":\n",
    "        list_word.append(len(word))\n",
    "        word = \"\"\n",
    "    else:\n",
    "        word += char\n",
    "\n",
    "print(list_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "# 参考回答\n",
    "raw_text = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics\"\n",
    "\n",
    "# replace('置換前', '置換後')\n",
    "text = raw_text.replace('.', ' ').replace(',', ' ')\n",
    "# split(): 指定なしで反核で切る\n",
    "ans = [len(w) for w in text.split()]\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. 元素記号 \n",
    "“Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.”という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭の2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 1, 'He': 2, 'Li': 3, 'Be': 4, 'B': 5, 'C': 6, 'N': 7, 'O': 8, 'F': 9, 'Ne': 10, 'Na': 11, 'Mi': 12, 'Al': 13, 'Si': 14, 'P': 15, 'S': 16, 'Cl': 17, 'Ar': 18, 'K': 19, 'Ca': 20}\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can\"\n",
    "word_list = [w for w in text.replace('.', ' ').replace(',', ' ').split()]\n",
    "arry = [1, 5, 6, 7, 8, 9, 15, 16, 19]\n",
    "\n",
    "word =[]\n",
    "index = [i for i in range(1, len(word_list)+1)]\n",
    "\n",
    "for atom in range(len(word_list)):\n",
    "    if atom+1 in arry:\n",
    "        word.append(word_list[atom][0])\n",
    "    else:\n",
    "        word.append(word_list[atom][:2])\n",
    "\n",
    "# zip関数で、要素をまとめてる\n",
    "print(dict(zip(word,index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 1, 'He': 2, 'Li': 3, 'Be': 4, 'B': 5, 'C': 6, 'N': 7, 'O': 8, 'F': 9, 'Ne': 10, 'Na': 11, 'Mi': 12, 'Al': 13, 'Si': 14, 'P': 15, 'S': 16, 'Cl': 17, 'Ar': 18, 'K': 19, 'Ca': 20}\n"
     ]
    }
   ],
   "source": [
    "# 参考回答\n",
    "def extract_chars(i, word):\n",
    "    if i in [1, 5, 6, 7, 8, 9, 15, 16, 19]:\n",
    "        return (word[0], i)\n",
    "    else:\n",
    "        return (word[:2], i)\n",
    "\n",
    "# 文を単語に分割\n",
    "raw_text = 'Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.'\n",
    "text = raw_text.replace('.', '').replace(',', '')\n",
    "# 文字を取り出す\n",
    "    # enumerateを使って、インデックス情報を取得、第二引数で０以外から開始する\n",
    "ans = [extract_chars(i, w) for i, w in enumerate(text.split(), 1)]\n",
    "# dictを用いて辞書型のデータに変換\n",
    "print(dict(ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. n-gram\n",
    "与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，”I am an NLPer”という文から単語bi-gram，文字bi-gramを得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "['I', 'am', 'an', 'NLPer']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am an NLPer\"\n",
    "\n",
    "def n_gram1(text, type):\n",
    "    if type == \"C\":\n",
    "        list_1 = [w for w in text]\n",
    "        return list_1\n",
    "    if type == \"w\":\n",
    "        text_2 = text.split(' ')\n",
    "        return text_2\n",
    "\n",
    "print(n_gram1(text, \"c\"))\n",
    "print(n_gram1(text, \"w\"))\n",
    "\n",
    "# よく分からないのでまた後でやる\n",
    "# これが参考になりそう：https://qiita.com/kazmaw/items/4df328cba6429ec210fb\n",
    "\"\"\"\n",
    "N-gramとは\n",
    "    任意の文字列や文書を連続したnこの文字で分割する方法\n",
    "    n=1: uni-gram, n=2: bi-gram, n=3: tri-gram\n",
    "    ex. I an an NLPer\n",
    "    uni-gram:==> 'I', ' ', 'a', 'm', ' ', 'a', 'n', ' ', 'N', 'L', 'P', 'e', 'r'\n",
    "    bi-gram: ==> \n",
    "    単語N-gramであれば、単語単位で区切る\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'a', 'm', ' ', 'a', 'n', ' ', 'N', 'L', 'P', 'e', 'r']\n",
      "[['I'], ['am'], ['an'], ['NLPer']]\n",
      "['I ', ' a', 'am', 'm ', ' a', 'an', 'n ', ' N', 'NL', 'LP', 'Pe', 'er']\n",
      "[['I', 'am'], ['am', 'an'], ['an', 'NLPer']]\n",
      "['I a', ' am', 'am ', 'm a', ' an', 'an ', 'n N', ' NL', 'NLP', 'LPe', 'Per']\n",
      "[['I', 'am', 'an'], ['am', 'an', 'NLPer']]\n"
     ]
    }
   ],
   "source": [
    "def n_gram(target, n):\n",
    "    #基準を1文字ずつずらしながらn文字抜き出している\n",
    "    return [target[idx:idx + n] for idx in range(len(target) - n + 1)]\n",
    "\n",
    "\n",
    "text = 'I am an NLPer'\n",
    "for i in range(1, 4):\n",
    "    print(n_gram(text, i))\n",
    "    print(n_gram(text.split(' '), i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. 集合\n",
    "“paraparaparadise”と”paragraph”に含まれる文字bi-gramの集合を，それぞれ, XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，’se’というbi-gramがXおよびYに含まれるかどうかを調べよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b8b7b4f0ecb4fe488c708522ab2b3d7dd4520241d9f917dd3cf1964317173b9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('.nlp100': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
